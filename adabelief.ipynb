{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npath = os.path.join(os.getcwd(), 'Best_trained_models')        \nif not os.path.exists(path):\n    os.mkdir(path)\n    \npath = os.path.join(os.getcwd(), 'Plot_curves')        \nif not os.path.exists(path):\n    os.mkdir(path)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **optimizer.py**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.optim import Optimizer\n\n\nclass AdaBelief(Optimizer):\n    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=5e-4,\n                 weight_decouple=True, fixed_decay=False, correct_bias=True):\n\n        if lr < 0:\n            raise ValueError(\"Invalid learning rate\")\n        if not 0 <= betas[0] <= 1:\n            raise ValueError(\"Invalid beta 0\")\n        if not 0 <= betas[1] <= 1:\n            raise ValueError(\"Invalid beta 1\")\n        if not 0 <= eps:\n            raise ValueError(\"Invalid epsilon\")\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                       correct_bias=correct_bias)\n\n        super(AdaBelief, self).__init__(params, defaults)\n        self.weight_decouple = weight_decouple\n        self.fixed_decay = fixed_decay\n    \n    def reset(self):\n        for group in self.param_groups:\n            for param in group['params']:\n                state = self.state[param]\n                state['step']=0\n                state['exp_avg'] = torch.zeros_like(param.data,memory_format=torch.preserve_format)\n                state['exp_avg_sq'] = torch.zeros_like(param.data,memory_format=torch.preserve_format)\n\n    def step(self, closure=None):\n        for group in self.param_groups:\n            for param in group['params']:\n                if param.grad is None:\n                    continue\n                grad = param.grad.data\n\n                if grad.is_sparse:\n                    raise RuntimeError('Grads are sparse')\n\n                state = self.state[param]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(param.data)\n                    state['exp_avg_sq'] = torch.zeros_like(param.data)\n\n#                 Weight Decay    \n                if self.weight_decouple:\n                    if not self.fixed_decay:\n                        param.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n                    else:\n                        param.data.mul_(1.0 - group['weight_decay'])\n                else:\n                    if group['weight_decay'] != 0:\n                        grad.add_(param.data, alpha=group['weight_decay'])\n                \n                \n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta_1, beta_2 = group['betas']\n                state['step'] += 1\n\n                m_t = torch.add(torch.mul(beta_1, exp_avg), torch.mul(1.0 - beta_1, grad))\n                # s_t = torch.add(torch.mul(beta_2, exp_avg_sq) , torch.mul(1.0-beta_2,torch.square(grad)) )\n                s_t = torch.add(torch.add(torch.mul(beta_2, exp_avg_sq),\n                                          torch.mul(1.0 - beta_2, torch.square(torch.sub(grad, m_t)))), group['eps'])\n\n                if group['correct_bias']:\n                    m_t_hat = m_t.divide(1.0 - beta_1 ** state['step'])\n                    s_t_hat = s_t.divide(1.0 - beta_2 ** state['step'])\n\n                denom = torch.add(torch.sqrt(s_t_hat), group['eps'])\n                param.data.addcdiv_(m_t_hat, denom, value=-group['lr'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **models.py**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport operator\nimport functools\nimport math\n\nclass VGG(nn.Module):\n    '''\n    VGG model\n    '''\n\n    def __init__(self, architecture, num_classes=100, input_dims=[3, 32, 32]):\n        super(VGG, self).__init__()\n        self.architecture = architecture\n        self.num_classes = num_classes\n        self.input_dims = input_dims\n        self.convs = self.conv()\n        self.fcs = self.fc()\n        \n#         for m in self.modules():\n#             if isinstance(m, nn.Conv2d):\n#                 n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n#                 m.weight.data.normal_(0, math.sqrt(2. / n))\n#                 m.bias.data.zero_()\n\n    def conv(self):\n        layers = []\n        input_channels = self.input_dims[0]\n\n        for value in self.architecture:\n            if type(value) == int:\n                layers += [nn.Conv2d(in_channels=input_channels, out_channels=value, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(value),nn.ReLU(inplace=True)]\n                input_channels = value\n            else:\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n\n        return nn.Sequential(*layers)\n\n    def fc(self):\n\n        features_size = functools.reduce(operator.mul, list(self.convs(torch.rand(1, *self.input_dims)).shape))\n\n        return nn.Sequential(nn.Dropout(p=0.5),\n                            nn.Linear(features_size, 512),\n                             nn.ReLU(inplace=True),\n                             nn.Dropout(p=0.5),\n                             nn.Linear(512, 512),\n                             nn.ReLU(inplace=True),\n                             \n                             nn.Linear(512, self.num_classes)\n                             )\n\n    def forward(self, x):\n        x = self.convs(x)\n        x = x.view(x.size(0), -1)\n        x = self.fcs(x)\n        return x\n\n    \n# Used inside ResNet\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                     padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=100):\n        super().__init__()\n        \n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 , num_classes)\n\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None  \n   \n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes, 1, stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        \n        self.inplanes = planes\n        \n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n    \n    \n    def forward(self, x):\n        x = self.conv1(x)           \n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)         \n\n        x = self.layer1(x)          \n        x = self.layer2(x)          \n        x = self.layer3(x)          \n        x = self.layer4(x)          \n\n        x = self.avgpool(x)         \n        x = torch.flatten(x, 1)     \n        x = self.fc(x)\n\n        return x\n# model = ResNet(BasicBlock,[3,4,6,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# main.py","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nfrom torch import optim, nn\n# from optimizer import AdaBelief\n# from models import VGG\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision\nfrom torch.utils.data import DataLoader\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef adjust_learning_rate(optimizer,gamma=0.1,reset=True):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] *= gamma\n    if optimizer.__class__.__name__ == 'AdaBelief' and reset:\n        optimizer.reset()\n    elif optimizer.__class__.__name__ == 'Adam' and reset:\n        for group in optimizer.param_groups:\n            for param in group['params']:\n                state = optimizer.state[param]\n                state['step']=torch.zeros((1,),dtype=torch.float, device=param.device)\n                state['exp_avgs'] = torch.zeros_like(param.data,memory_format=torch.preserve_format)\n                state['exp_avg_sq'] = torch.zeros_like(param.data,memory_format=torch.preserve_format)\n    elif optimizer.__class__.__name__ == 'SGD' and reset:\n        for group in optimizer.param_groups:\n            for param in group['params']:\n                state = optimizer.state[param]\n                state['step']=0\n                state['momentum_buffer'] = torch.zeros_like(param.data,memory_format=torch.preserve_format)\n                \n        \ndef initialize_optimizer(inp_model, optimizer='SGD'):\n    if optimizer == 'Adam':\n        return optim.Adam(inp_model.parameters(), lr=0.001,weight_decay=5e-4)\n    elif optimizer == 'SGD':\n        return optim.SGD(inp_model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n    elif optimizer == 'AdaBelief':\n        return AdaBelief(inp_model.parameters(), lr=0.0001)\n\n\ndef build_model(model_type):\n    network = None\n    if model_type == \"VGG\":\n        VGG11 = [64, \"MP\", 128, \"MP\", 256, 256, \"MP\", 512, 512, \"MP\", 512, 512, \"MP\"]\n        network = VGG(VGG11).to(device)\n    elif model_type == 'ResNet':\n        layers = [3,4,6,4]\n        network = ResNet(BasicBlock, layers).to(device)\n    if device == 'cuda':\n        network = torch.nn.DataParallel(network)\n\n    return network\n\n\ndef cross_entropy_loss_function():\n    return nn.CrossEntropyLoss()\n\n\ndef get_data(batch_size=128):\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    cifar_train_data = torchvision.datasets.CIFAR100(root='./data/', train=True,\n                                                    download=True, transform=transform_train)\n\n    cifar_test_data = torchvision.datasets.CIFAR100(root='./data/', train=False,\n                                                   download=True, transform=transform_test)\n\n    cifar_train_loader = DataLoader(cifar_train_data, batch_size=batch_size, shuffle=True)\n    cifar_test_loader = DataLoader(cifar_test_data, shuffle=False, batch_size=batch_size)\n\n    return cifar_train_loader, cifar_test_loader\n\n\ndef test(net, test_data, criterion):\n    correct = 0\n    total = 0\n    test_loss = 0\n\n    net.eval()\n\n    with torch.no_grad():\n        for data in test_data:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Test accuracy {accuracy}%\")\n\n    return accuracy, test_loss\n\n\ndef train(net, epoch, train_data, optimizer, criterion):\n    net.train()\n    correct = 0\n    total = 0\n    train_loss = 0.0\n\n    print('\\nEpoch: %d' % epoch)\n\n    for i, data in enumerate(train_data):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f\"Training accuracy {accuracy}%\")\n\n    return accuracy, train_loss\n\n\ndef main(dataset, model_architecture, init_optimizer):\n    train_loader, test_loader = get_data()\n\n    net = build_model(model_architecture)\n    criterion = cross_entropy_loss_function()\n    optimizer = initialize_optimizer(net, init_optimizer)\n\n    start = 1\n    end = 200\n    best_acc = 0\n    train_accuracies = []\n    test_accuracies = []\n    train_loss_trends = []\n    test_loss_trends = []\n\n    for epoch in range(start, end+1):\n\n        if epoch==150:\n            adjust_learning_rate(optimizer,reset=False)\n        train_acc, train_loss = train(net, epoch, train_loader, optimizer, criterion)\n        test_acc, test_loss = test(net, test_loader, criterion)\n\n        if test_acc > best_acc:\n            state = {\n                'net': net.state_dict(),\n                'acc': test_acc,\n                'epoch': epoch,\n            }\n            file_path = os.path.join(os.getcwd() + \"/Best_trained_models/\"+f\"{dataset}_{model_architecture}_{init_optimizer}.pt\" )\n            torch.save(state,file_path)\n            best_acc = test_acc\n\n        train_accuracies.append(train_acc)\n        test_accuracies.append(test_acc)\n        train_loss_trends.append(train_loss)\n        test_loss_trends.append(test_loss)\n\n    pickle.dump({'train_acc': train_accuracies, 'test_acc': test_accuracies, 'train_loss': train_loss_trends,\n                 'test_loss': test_loss_trends}, open(\n        os.path.join(os.getcwd() + \"/Plot_curves\",  f\"{dataset}_{model_architecture}_{init_optimizer}.p\"),\"wb\"))\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main(\"CIFAR-10\", \"ResNet\", \"SGD\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(files['CIFAR-10_ResNet_AdaBelief'],open('./Pickle3/CIFAR-10_ResNet_AdaBelief.p','wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport os\nimport matplotlib.pyplot as plt\n\nfile_names = [name for name in os.listdir('./Plot_curves')]\nprint(file_names)\nfiles = {}\nfor names in file_names:\n    files[names.split(\".\")[0]] = pickle.load(open('./Plot_curves/' + names , \"rb\"))\nfiles['CIFAR-10_ResNet_SGD']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}